{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "from app.data.services import BaseDataSource, IAbstractDataSource\n",
    "\n",
    "from typing import Union, List, Dict, Tuple\n",
    "\n",
    "\n",
    "class OpenDataSource(BaseDataSource, IAbstractDataSource):\n",
    "\n",
    "    VERSIONS = [1, 2]\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        name = \"OpenData1\"\n",
    "        url = \"http://www.datiopen.it/SpodCkanApi/api/\"\n",
    "        super().__init__(name, url)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_format_from_data(data: Dict) -> Union[str, None]:\n",
    "        url = data.get('url', None)\n",
    "        if not url: return url\n",
    "        format_ = data['url'].split('.')[-1]\n",
    "        return format_\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def parse_data_from_url(data_url: str) -> Tuple[str, str, str]:\n",
    "        \n",
    "        try:\n",
    "            data = requests.get(data_url).json()\n",
    "            title = data['title']\n",
    "            notes = data['notes']\n",
    "            tags = data['tags']\n",
    "        except Exception as e:\n",
    "            print(f\"Level 1: {e} : {url}\")\n",
    "            return '', '', ''\n",
    "        \n",
    "        df_metadata = OpenDataSource._add_tags(title, notes, tags)      \n",
    "\n",
    "        if \"resources\" in data.keys(): \n",
    "            list_info = data['resources']\n",
    "        elif \"relations\" in data.keys(): \n",
    "            list_info = data['relations']\n",
    "        else:\n",
    "            print(f\"{data_url} not found valid data key\") \n",
    "            return '', '', ''\n",
    "\n",
    "        url = ''\n",
    "        for resource in list_info:\n",
    "            format_ = OpenDataSource.parse_format_from_data(resource)\n",
    "            if format_ == 'csv':\n",
    "                url = resource['url']\n",
    "\n",
    "        return url, title, df_metadata\n",
    "\n",
    "\n",
    "    def parse_url_from_version(self, version: Union[int, str]) -> str:\n",
    "        return self.url + f\"{version}/rest/dataset\"\n",
    "    \n",
    "    def _list_available_datasets(self, *args, **kwargs) -> List[str]:\n",
    "        # get alla avilable datasets names\n",
    "        nested_datasets = [requests.get(self.parse_url_from_version(version)).json() for version in OpenDataSource.VERSIONS]\n",
    "        # save version to datasets\n",
    "        setattr(self, 'dfs', nested_datasets)\n",
    "        # flatten the list\n",
    "        datasets_names = [item for sublist in nested_datasets for item in sublist]\n",
    "\n",
    "        return datasets_names\n",
    "\n",
    "\n",
    "    def _get_metadata(self, \n",
    "                      dataset_names: Union[List[str], str],\n",
    "                      data_source_name: str,\n",
    "                      *args, **kwargs) -> Dict[str, List[str]]:\n",
    "        \n",
    "        metadata = OpenDataSource._create_metadata()\n",
    "        for name in dataset_names[:50]:\n",
    "\n",
    "            dataset_version = [name in version_names for version_names in self.dfs].index(True) + 1\n",
    "            dataset_url = self.parse_url_from_version(dataset_version) + '/' + name\n",
    "\n",
    "            url, title, df_metadata = OpenDataSource.parse_data_from_url(dataset_url)\n",
    "\n",
    "            if (url == '') and (title == '') and (df_metadata == ''): continue \n",
    "                \n",
    "            metadata = OpenDataSource._update_metadata(metadata, \n",
    "                                                       url=url,\n",
    "                                                       name=title,\n",
    "                                                       text=df_metadata,\n",
    "                                                       data_source=data_source_name)            \n",
    "            print(f\"{name} downloaded\")\n",
    "            \n",
    "\n",
    "        return metadata.dict()\n",
    "\n",
    "\n",
    "    def read_dataset(self, \n",
    "                     dataset_url: Union[List[str], str], \n",
    "                     *args, **kwargs) -> DataFrame:\n",
    "        \n",
    "        separator = BaseDataSource.infer_separetor(dataset_url)\n",
    "        \n",
    "        return read_csv(dataset_url, sep=separator, encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_open = OpenDataSource()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(obj_open)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_open",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
